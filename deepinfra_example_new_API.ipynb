{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "sRDoubD36OVo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# #Your API key here\n",
        "# API_KEY = DEEP_INFRA_API_KEY\n",
        "\n",
        "# API_ENDPOINT = 'https://api.deepinfra.com/v1/openai/chat/completions'"
      ],
      "metadata": {
        "id": "LFoxGaVJifTV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "LLM_API = userdata.get('deep_infra_llm')"
      ],
      "metadata": {
        "id": "s7wBZqLtitoF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_0k2XQ6AVi",
        "outputId": "70131ce0-b8f1-48cc-c0b2-9b57e07f8f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! That's a nice double hello! How are you today?\n",
            "13 14\n"
          ]
        }
      ],
      "source": [
        "# Assume openai>=1.0.0\n",
        "from openai import OpenAI\n",
        "\n",
        "# Create an OpenAI client with your deepinfra token and endpoint\n",
        "openai = OpenAI(\n",
        "    api_key=f\"{LLM_API}\",\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        ")\n",
        "\n",
        "chat_completion = openai.chat.completions.create(\n",
        "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"},\n",
        "              {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "              ],\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n",
        "print(chat_completion.usage.prompt_tokens, chat_completion.usage.completion_tokens)\n",
        "\n",
        "# Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
        "# 11 25"
      ]
    }
  ]
}